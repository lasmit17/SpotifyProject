{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHybg7NVL6DEcQfbK+7HBp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lasmit17/SpotifyProject/blob/master/XGBoost_Collab_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dPqI9D2YG_U4"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc, average_precision_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "FUrWrHowJVzC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luke_df_2016 = pd.read_csv(f'Luke_2016_Top_Songs.csv')\n",
        "luke_df_2017 = pd.read_csv(f'Luke_2017_Top_Songs.csv')\n",
        "luke_df_2018 = pd.read_csv(f'Luke_2018_Top_Songs.csv')\n",
        "luke_df_2019 = pd.read_csv(f'Luke_2019_Top_Songs.csv')\n",
        "luke_df_2020 = pd.read_csv(f'Luke_2020_Top_Songs.csv')\n",
        "\n",
        "jp_df_2017 = pd.read_csv(f'jp_2017_Top_Songs.csv')\n",
        "jp_df_2018 = pd.read_csv(f\"jp_2018_Top_Songs.csv\")\n",
        "jp_df_2019 = pd.read_csv(f\"jp_2019_Top_Songs.csv\")\n",
        "jp_df_2020 = pd.read_csv(f\"jp_2020_Top_Songs.csv\")\n",
        "\n",
        "fabian_df_2018 = pd.read_csv(f\"FP_2018_Top_Songs.csv\")\n",
        "fabian_df_2019 = pd.read_csv(f\"fp_2019_Top_Songs.csv\")\n",
        "fabian_df_2020 = pd.read_csv(f\"FP_2020_Top_Songs.csv\")\n",
        "fabian_df_2017 = pd.read_csv(f\"FP_2017_Top_Songs.csv\")"
      ],
      "metadata": {
        "id": "nZRvPLZ1Ijdx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_years(df):\n",
        "    years = []\n",
        "    for date in df['release_date'].values:\n",
        "        if '-' in date:\n",
        "            years.append(date.split('-')[0])\n",
        "        else:\n",
        "            years.append(date)\n",
        "    df['release_year'] = years\n",
        "    return df"
      ],
      "metadata": {
        "id": "MdGkM1KFJNZD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "luke_df_2016 = get_years(luke_df_2016)\n",
        "luke_df_2017 = get_years(luke_df_2017)\n",
        "luke_df_2018 = get_years(luke_df_2018)\n",
        "luke_df_2019 = get_years(luke_df_2019)\n",
        "luke_df_2020 = get_years(luke_df_2020)\n",
        "\n",
        "jp_df_2017 = get_years(jp_df_2017)\n",
        "jp_df_2018 = get_years(jp_df_2018)\n",
        "jp_df_2019 = get_years(jp_df_2019)\n",
        "jp_df_2020 = get_years(jp_df_2020)\n",
        "\n",
        "fabian_df_2018 = get_years(fabian_df_2018)\n",
        "fabian_df_2019 = get_years(fabian_df_2019)\n",
        "fabian_df_2020 = get_years(fabian_df_2020)\n",
        "fabian_df_2017 = get_years(fabian_df_2017)"
      ],
      "metadata": {
        "id": "2zVNR4sOJjZX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining each person's data\n",
        "luke_df_concat = pd.concat([luke_df_2016, luke_df_2017, luke_df_2018, luke_df_2019, luke_df_2020], ignore_index=True, axis=0)\n",
        "luke_df_concat['users_name'] = \"Luke\"\n",
        "\n",
        "jp_df_concat = pd.concat([jp_df_2017, jp_df_2018, jp_df_2019, jp_df_2020], ignore_index=True, axis=0)\n",
        "jp_df_concat['users_name'] = \"JP\"\n",
        "\n",
        "fabian_df_concat = pd.concat([fabian_df_2017, fabian_df_2018, fabian_df_2019, fabian_df_2020], ignore_index=True, axis=0)\n",
        "fabian_df_concat['users_name'] = \"Fabian\"\n",
        "\n",
        "#Combining every data frame together into one\n",
        "all_df = pd.concat([luke_df_concat, jp_df_concat, fabian_df_concat], ignore_index=True, axis=0)\n",
        "\n",
        "all_df[\"release_year\"] = pd.to_numeric(all_df[\"release_year\"])"
      ],
      "metadata": {
        "id": "EcEvwaf1JnOw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_scale(df, col):\n",
        "    df[col + '_old'] = df[col]\n",
        "    new_max = 1\n",
        "    new_min = 0\n",
        "    new_range = new_max-new_min\n",
        "    max_val = df[col].max()\n",
        "    min_val=df[col].min()\n",
        "    val_range = max_val - min_val\n",
        "    df[col]=df[col].apply(lambda x: (((x-min_val)*new_range)/val_range)+new_min)\n",
        "    return\n",
        "\n",
        "#Setting the numerical spotify features\n",
        "numeric_spotify_features = ['energy',\n",
        "    'valence',\n",
        "    'danceability',\n",
        "    'liveness',\n",
        "    'speechiness',\n",
        "    'instrumentalness',\n",
        "    'acousticness',\n",
        "    'loudness',\n",
        "    'length',\n",
        "    'popularity',\n",
        "    'tempo',\n",
        "    'release_year']\n",
        "\n",
        "\n",
        "for col in numeric_spotify_features:\n",
        "    convert_scale(all_df, col)\n",
        "\n",
        "#Now we need to address the categorical variables. First, we will utilize One Hot Encoder\n",
        "ohe = OneHotEncoder()\n",
        "ohe_results = ohe.fit_transform(all_df[['time_signature', 'mode', 'key']])\n",
        "onehot_df = pd.DataFrame(ohe_results.toarray(), columns=['time_signature_1', 'time_signature_2',\n",
        "                                                     'time_signature_3', 'time_signature_4',\n",
        "                                                     'mode_1', 'mode_2',\n",
        "                                                     'key_1', 'key_2', 'key_3', 'key_4', 'key_5', 'key_6',\n",
        "                                                     'key_7', 'key_8', 'key_9', 'key_10', 'key_11', 'key_12'])\n",
        "all_df = pd.concat([all_df, onehot_df], axis=1)"
      ],
      "metadata": {
        "id": "G8SVnoh-Jqp2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_spotify_features = ['energy',\n",
        "    'valence',\n",
        "    'danceability',\n",
        "    'liveness',\n",
        "    'speechiness',\n",
        "    'instrumentalness',\n",
        "    'acousticness',\n",
        "    'loudness',\n",
        "    'length',\n",
        "    'popularity',\n",
        "    'tempo',\n",
        "    'release_year',\n",
        "    'time_signature_1',\n",
        "    'time_signature_2',\n",
        "    'time_signature_3',\n",
        "    'time_signature_4',\n",
        "    'mode_1',\n",
        "    'mode_2',\n",
        "    'key_1',\n",
        "    'key_2',\n",
        "    'key_3',\n",
        "    'key_4',\n",
        "    'key_5',\n",
        "    'key_6',\n",
        "    'key_7',\n",
        "    'key_8',\n",
        "    'key_9',\n",
        "    'key_10',\n",
        "    'key_11',\n",
        "    'key_12']"
      ],
      "metadata": {
        "id": "LHb5M_h0JvmT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df[\"users_name\"] = all_df['users_name'].replace({\"Luke\":2,\"JP\":1,\"Fabian\":0})\n",
        "\n",
        "X = all_df[all_spotify_features]\n",
        "y = all_df['users_name']\n",
        "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
        "\n",
        "xg_reg = xgb.XGBClassifier(objective ='multi:softmax', colsample_bytree = 0.3, learning_rate = 0.1,\n",
        "                max_depth = 5, alpha = 10, n_estimators = 10, num_classes = 3)\n",
        "xg_reg.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "pred_xgb = xg_reg.predict(X_test)\n",
        "\n",
        "mse_xgb = np.sqrt(mean_squared_error(y_test, pred_xgb))\n",
        "print(\"MSE: %f\" % (mse_xgb))\n",
        "\n",
        "xgb_initial_score = cross_val_score(xg_reg, X, y, cv=10)\n",
        "xgb_initial_score\n",
        "\n",
        "xgb_initial_accuracy = accuracy_score(y_test, pred_xgb)\n",
        "xgb_initial_accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxe9dM_1JzSq",
        "outputId": "9ca5a389-c326-49bb-f15b-242ebf4528fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.938083\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6738461538461539"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}